# -*- coding: utf-8 -*-
"""clasificacion 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RUphbeqwKBZJ8AMSmUVjexKNE_0RBL3r

# Diagnóstico de cáncer de mama

El objetivo es clasificar un tumor en Maligno (M) o Benigno (B) a partir de un conjunto de datos que contiene características extraídas de la imagen digitalizada de una biopsia de bulto en mama. Se dispone de 569 muestras, 357 de ellas benignas y 212 malignas. Cada
muestra posee una identificación (ID number), el diagnóstico (diagnosis) y 30 atributos relacionados a 10 características:

1. radius: media de las distancias desde el centro hacia puntos del perímetro.
2. texture: desviación estándar de las intensidades en escala de grises.
3. perimeter.
4. area.
5. smoothness: variación local de largo de los radios.
6. compactness: perimeter2 / area - 1.0
7. concavity: severidad de secciones cóncavas en la delineación.
8. concave points: número de secciones cóncavas en la delineación.
9. symmetry.
10. fractal dimension: "coastline approximation" - 1

# Hito N° 2

Descripción del o los modelos predictivos a utilizar, la metodología utilizada para ajustarlos, resultados y conclusión.

# Desarrollo

## 1. Preprocesamiento

Como buena práctica antes de ajustar un módelo de aprendizaje (no) supervisado se debe preprocesar los datos, para lo cual se recomienda efectuar al menos las siguientes tareas:

1. Análisis exploratorio de los datos, estadísticas básicas.
2. Verificar la cantidad de datos ausentes (missing values)
3. Imputación de datos ausentes (KNN, media, ect)
4. Tratamiento de variables categóricas (OneHotEnconding, dummies)
5. Identificación e imputación de datos atípicos.
6. Análisis de multicolinealidad de los datos (Análisis de correlación, diagramas de caja, dispersión, etc)
7. Selección de variables (características, features, campos, columnas)
8. Escalamieto de los datos
9. División de los datos en conjunto de entrenamiento, prueba/validación.

### 1.1 Importación de los datos
"""

# pandas es un módulo que contiene un conjunto de clases
# y funciones para trabajar con datos tabulares o estructurados.
import pandas as pd

# La función read_csv lee un archivo csv y lo carga en memoria como
# una instancia del objeto dataframe en memoria ram.
dataset = pd.read_csv('data_breast_cancer.csv', sep=',')
dataset.shape

# La función head entrega las n primeras filas del dataframe
dataset.head(3)

# La función info entrega 3 campos, el nombre de la variable, la cantidad de valores
# no nulos y nulos, junto con el tipo de variable, ojo que object es todo tipo de
# datos que no es númerico, como lo son las cadenas de texto
dataset.info()

"""- Eliminaremos las columnas <code>id</code> y <code>Unnamed:32</code>"""

# Drop elimina las columnas/variables/features indicadas en la lista, el
# axis si es 0 indica que se considera el index y 1 para las columnas
# inplace true indica que se efectue la operación y se actualice el dataframe
dataset.drop(['id','Unnamed: 32'], axis=1, inplace=True)
dataset.info()

"""### 1.2 Detección de datos ausentes

- No existen omisiones en los datos, conforme al resultado de la función <code>dataset.info()</code> de la clase <code>Dataframe</code> del módulo <code>pandas</code>

### 1.3 Modificación variable objetivo

- Cambiaremos el tipo de dato de la variable objetivo, a binaria, 1 si el diagnóstico es M y 0 en otro caso.
"""

def to_binary(x):
    if x == 'M':
        return 1
    else:
        return 0

# La función apply indica que se aplique la función to_binary a todos los valores de la columna
# diagnosis
dataset.diagnosis = dataset.diagnosis.apply(to_binary)

# values_counts cuenta la frecuencia por cada valor único en la columna
dataset.diagnosis.value_counts()

"""- La clase 0 indica que la persona presenta un diagnostico benigno.
- La clase 1 indica que la persona presenta un diagnostico maligno.

**NOTA:** Siempre verificar el balanceo de la variable objetivo, en este caso los valores son cercanos por lo que no tendremos inconvenientes. En cambio, por ejemplo si tengo que de 10 personas solo una sobrevive, existe un problema de desbalanceo de clases y se debe aplicar técnicas que permitan eliminar ese sesgo, buscar en google oversampling.

### 1.4 Datos atípicos

Se buscarán los puntos atípicos en el set de datos y se imputarán mediante la técnica de distribución de cuartiles.
"""

# Plotly es el módulo de gráfico más versátil de python, ya que se puede integrar
# con aplicaciones web, por ejemplo usando DASH puedo gráficar los predictores
# de un módelo y visualizar como este cambia al modificar los parámetros.

import plotly.graph_objects as go
from plotly.subplots import make_subplots

#Se crea el marco de dos filas por 5 columnas para colocar gráficos de caja
fig = make_subplots(rows=2,cols=5)

# Se agregan los gráficos de caja al marco anterior, donde row y col indican cual es la posición.
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.radius_mean, name='radius_mean'), row=1, col=1)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.texture_mean, name='texture_mean'), row=1, col=2)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.perimeter_mean, name='perimeter_mean'), row=1, col=3)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.area_mean, name='area_mean'), row=1, col=4)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.smoothness_mean, name='smoothness_mean'), row=1, col=5)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.compactness_mean, name='compactness_mean'), row=2, col=1)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset['concave points_mean'], name='concave points_mean'), row=2, col=2)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.symmetry_mean, name='symmetry_mean'), row=2, col=3)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.fractal_dimension_mean, name='fractal_dimension_mean'), row=2, col=4)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.concavity_mean, name='concavity_mean'), row=2, col=5)

# Con esto se setea el título del gráfico y
# Se definen los márgenes
fig.update_layout(
    title = go.layout.Title(text='Boxplot Grupo Medias'),
    margin=dict(l=20, r=20, t=50, b=20)
)

# Estando listo las configuraciones, se despliega el gráfico.
fig.show()

fig2 = make_subplots(rows=2,cols=5)

fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.radius_se, name='radius_se'), row=1, col=1)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.texture_se, name='texture_se'), row=1, col=2)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.perimeter_se, name='perimeter_se'), row=1, col=3)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.area_se, name='area_se'), row=1, col=4)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.smoothness_se, name='smoothness_se'), row=1, col=5)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.compactness_se, name='compactness_se'), row=2, col=1)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset['concave points_se'], name='concave points_se'), row=2, col=2)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.symmetry_se, name='symmetry_se'), row=2, col=3)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.fractal_dimension_se, name='fractal_dimension_se'), row=2, col=4)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.concavity_se, name='concavity_se'), row=2, col=5)

fig2.update_layout(
    title = go.layout.Title(text='Boxplot Grupo SE'),
    margin=dict(l=20, r=20, t=50, b=20)
)

fig2.show()

fig3 = make_subplots(rows=2,cols=5)

fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.radius_worst, name='radius_worst'), row=1, col=1)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.texture_worst, name='texture_worst'), row=1, col=2)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.perimeter_worst, name='perimeter_worst'), row=1, col=3)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.area_worst, name='area_worst'), row=1, col=4)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.smoothness_worst, name='smoothness_worst'), row=1, col=5)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.compactness_worst, name='compactness_worst'), row=2, col=1)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset['concave points_worst'], name='concave points_worst'), row=2, col=2)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.symmetry_worst, name='symmetry_worst'), row=2, col=3)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.fractal_dimension_worst, name='fractal_dimension_worst'), row=2, col=4)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.concavity_worst, name='concavity_worst'), row=2, col=5)

fig3.update_layout(
    title = go.layout.Title(text='Boxplot Grupo Worst'),
    margin=dict(l=20, r=20, t=50, b=20)
)

fig3.show()

# iloc permite filtrar filas y columnas mediante el índice [f0:fn,c0:cn], luego se aplica
# la función describe que entrega solo para datos númericos sus estadísticas
# básicas
dataset.iloc[:,1:32].describe()

"""- Podemos observar que existen datos atipicos, que pueden influir negativamente en los resultados de nuestro modelo, por lo que seran imputados.
- Se imputaran utilizando el método del rango intercuartil, que es la diferencia entre el tercer y primer cuartil de los datos.
"""

columns = dataset.iloc[:,1:32].columns
columns

# Función de imputación de datos atípicos
# Por cada columna del dataframe, efectúa los cálculos y reemplaza los valores atípicos
# por la media del conjunto completo, también, podría considerarse
# reemplazar por la media de los k vecinos más cercanos
import numpy as np
for column in columns:
    q25, q75 = np.percentile(dataset[column], 25), np.percentile(dataset[column], 75)
    iqr = q75 - q25
    cut_off = iqr * 1.5
    lower, upper = q25 - cut_off, q75 + cut_off
    dataset[column].replace(
        to_replace = [x for x in dataset[column] if x < lower or x > upper],
        value = dataset[column].median(),inplace=True #Aquí se podría aplicar KNN means replace
    )

fig = make_subplots(rows=2,cols=5)

fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.radius_mean, name='radius_mean'), row=1, col=1)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.texture_mean, name='texture_mean'), row=1, col=2)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.perimeter_mean, name='perimeter_mean'), row=1, col=3)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.area_mean, name='area_mean'), row=1, col=4)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.smoothness_mean, name='smoothness_mean'), row=1, col=5)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.compactness_mean, name='compactness_mean'), row=2, col=1)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset['concave points_mean'], name='concave points_mean'), row=2, col=2)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.symmetry_mean, name='symmetry_mean'), row=2, col=3)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.fractal_dimension_mean, name='fractal_dimension_mean'), row=2, col=4)
fig.add_trace(go.Box(x=dataset.diagnosis, y=dataset.concavity_mean, name='concavity_mean'), row=2, col=5)

fig.update_layout(
    title = go.layout.Title(text='Boxplot Grupo Medias'),
    margin=dict(l=20, r=20, t=50, b=20)
)

fig.show()

fig2 = make_subplots(rows=2,cols=5)

fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.radius_se, name='radius_se'), row=1, col=1)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.texture_se, name='texture_se'), row=1, col=2)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.perimeter_se, name='perimeter_se'), row=1, col=3)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.area_se, name='area_se'), row=1, col=4)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.smoothness_se, name='smoothness_se'), row=1, col=5)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.compactness_se, name='compactness_se'), row=2, col=1)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset['concave points_se'], name='concave points_se'), row=2, col=2)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.symmetry_se, name='symmetry_se'), row=2, col=3)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.fractal_dimension_se, name='fractal_dimension_se'), row=2, col=4)
fig2.add_trace(go.Box(x=dataset.diagnosis, y=dataset.concavity_se, name='concavity_se'), row=2, col=5)

fig2.update_layout(
    title = go.layout.Title(text='Boxplot Grupo SE'),
    margin=dict(l=20, r=20, t=50, b=20)
)

fig2.show()

fig3 = make_subplots(rows=2,cols=5)

fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.radius_worst, name='radius_worst'), row=1, col=1)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.texture_worst, name='texture_worst'), row=1, col=2)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.perimeter_worst, name='perimeter_worst'), row=1, col=3)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.area_worst, name='area_worst'), row=1, col=4)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.smoothness_worst, name='smoothness_worst'), row=1, col=5)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.compactness_worst, name='compactness_worst'), row=2, col=1)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset['concave points_worst'], name='concave points_worst'), row=2, col=2)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.symmetry_worst, name='symmetry_worst'), row=2, col=3)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.fractal_dimension_worst, name='fractal_dimension_worst'), row=2, col=4)
fig3.add_trace(go.Box(x=dataset.diagnosis, y=dataset.concavity_worst, name='concavity_worst'), row=2, col=5)

fig3.update_layout(
    title = go.layout.Title(text='Boxplot Grupo Worst'),
    margin=dict(l=20, r=20, t=50, b=20)
)

fig3.show()

"""### 1.5 Multicolinealidad

Análisis de correlatividad con el ojetivo de eliminar variables altamente correlacionadas. Como criterio se eliminaron aquellas variables sobre 0.9, dependiendo del criterio experto se pueden adoptar otras estrategías, como por ejemplo aplicar aleatoriamente varios algoritmos de árbol de decisión y calcular la importancia de los predictores para crear un ranking.
"""

import seaborn as sns # data visualization library
import matplotlib.pyplot as plt

corr = dataset.corr()
plt.figure(figsize=(18,18))
sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.1f',annot_kws={'size': 10},
           cmap= 'coolwarm')

"""- Se excluiran aquellas variables que contengan una alta correlación, solo manteniendo una de ellas.
 A eliminar

"""

# Creamos una copia del conjunto de datos
dataset_backup = dataset

dataset.columns

variables_eliminar = [
    'perimeter_mean', 'area_mean', 'concave points_mean', 'perimeter_se',
    'area_se', 'radius_worst', 'texture_worst', 'perimeter_worst',
    'area_worst', 'concave points_worst'
    ]

# Eliminamos las variables altamente correlacionadas
dataset.drop(variables_eliminar,axis=1, inplace=True)

# Volvemos a gráficar
corr = dataset.corr()
plt.figure(figsize=(18,18))
sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},
           cmap= 'coolwarm')

from sklearn.preprocessing import StandardScaler


col_names = dataset.iloc[:,1:32].columns
scaler = StandardScaler().fit(dataset.iloc[:,1:32].values)
features = scaler.transform(dataset.iloc[:,1:32].values)
dataset[col_names] = features

dataset.describe()

"""## 2. División de los datos

Mediante la función train_test_split se dividen los datos para entrenar y validar los modelos.
"""

from sklearn.model_selection import train_test_split

# Mis Xs son mis variables predictoras (features), de las cuales los algoritmos de predicción
# aprenden.
X = dataset.iloc[:,1:32]

# nuestra y es la variable objetivo
y = dataset['diagnosis']

# Se considera un 70% para entrenamiento y un 30% para prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)

"""## 3. Modelo de clasificación mediante Regresión Logística

La regresión logística es un método estadístico supervisado para predecir clases binarias, en este caso, tiene o no cáncer, cuyo resultado es de naturaleza dicotómica. Si se cumple los supuestos y se logra convergencia, el método de predicción mediante regresión logística resulta ser el más robusto.  

**Consideraciones para construir un modelo de regresión logística**
1. Variable objetivo debe ser dicotómica.
2. Los predictores son variables independientes.
3. Para tener un modelo más estable los predictores deben tener una distribución normal.
4. No debe existir multicolinealidad, ya que puede llevar a predicciones sesgadas. Se recomienda utilizar un diagrama de dispersión.
5. Verificar la normalidad multivariante.

**NOTA:** La desventaja de este modelo en la vida real es su costo de procesamiento y tiempo de ejecución, además la validación de los supuestos no siempre se cumple.

### 3.1 Ajuste de hiperparámetros

Generalmente, para entrenar un modelo, se deben definir parámetros, estos parámetros obedecen a
supuestos estadísticos, funciones de optimización, umbrales, márgenes de error, entre otros.


Mediante una búsquedad exhaustiva, se itera n veces en k validaciones cruzadas, para estimar cuales son los mejores parámetros y el mejor modelo de clasificación para nuestro conjunto de datos.

> https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV


# Se crea un instancia de la clase LogisticRegression que implementa el algoritmo de clasificación
# si observan no tiene parámetros
logModel = LogisticRegression()

# Mediante gridsearch se buscara la mejor combinación de estos parámetros
# se aconseja leer link de arriba sobre LogisticRegression
# para enteder que parámetros de deben definir
param_grid = [
    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],
    'C' : np.logspace(-4, 4, 20),
    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],
    'max_iter' : [1000, 2500, 5000]
    }
]

"""> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
GridSearchCV: Exhaustive search over specified parameter values for an estimator.
"""

# Se crea la instancia de la clase GridSearchCV que implementa lo que venimos hablando
# Se indica el modelo de clasificación, el diccionario de parámetros, cantidad de validaciones
# verbose para que muestre o no la info en consola del proceso
# y n_jobs -1 para que utilice todas las cpu
clf = GridSearchCV(
    logModel,
    param_grid = param_grid,
    cv = 10,
    verbose=False,
    n_jobs=-1
    )

# Una ves configurado todo, se ejecuta la busquedad exhaustiva.
# Dependendieno de la máquina, tomara su n tiempo.
best_clf = clf.fit(X,y)

"""A continución se obtiene el mejor modelo estimado por la búsqueda, el cual contiene la mejor evaluación obtenida, por ejemplo, analicemos su output:

One or more of the test scores are non-finite: [       nan        nan 0.62741228 ...        nan 0.94906015 0.94906015]

eso quiere decir que existieron combinaciones de parametros que obtuvieron un accuracy solo del 62%.
"""

# Propiedad de la clase GridSearchCV, el cual entrega el modelo mejor estimado.
best_clf.best_estimator_

"""Se econtró que el mejor modelo es LogisticRegression(C=0.23357214690901212, max_iter=1000)"""

# Guardamos nuestro modelo a otra variables
lr_model_clf = best_clf.best_estimator_

# Imprimimos el accuracy del modelo utilizando los datos de entrenamiento
print (f'Accuracy - : {lr_model_clf.score(X,y):.3f}')

# Imprimimos el accuracy del modelo utilizando los datos de prueba
print (f'Accuracy - : {lr_model_clf.score(X_test,y_test):.3f}')

"""### 3.2 Modelo Regresión Logística

Tomamos la mejor combinación de hiperparametros y ajustamos un nuevo modelo de clasificación mediante regresión logística.
"""

# Se crea una instancia del modelo con los siguientes parámetros
lr_model = LogisticRegression(C=0.23357214690901212, max_iter=1000)

# Ajustamos el modelo a nuestro datos, esta es la parte supervisada, a partir de nuestros datos
# el algoritmo aprende a como clasificar
lr_model_fit = lr_model.fit(X_train,y_train)

#Evaluamos el modelo entrenado sobre los datos de prueba
print(lr_model_fit.score(X_test,y_test))

"""## 4. Evaluación de un modelo de clasificación

Se define una función que a partir de una matriz de confusión calcula todas las métricas
que indican que tan bien se ajusta nuestro modelo al problema, tambien, sirve para compararlos con otros algoritmos.
"""

from numpy import sqrt

def model_diagnostic_stats(confusion_matrix):

    tp = confusion_matrix[1,1] # Número de resultados donde ambas pruebas son positivas; verdadero positivo
    tn = confusion_matrix[0,0] # Número de resultados donde el método candidato es positivo, pero el comparativo es negativo; verdadero negativo
    fp = confusion_matrix[0,1] # Número de resultados donde el método candidato es negativo, pero el comparativo es positivo; falso positivo
    fn = confusion_matrix[1,0] # Número de resultados donde ambos métodos son negativos. falso negativo

    p = tp + fn
    n = fp + tn
    pp = tp + fp
    pn = fn + tn

    #Diccionario de datos que contiene las métricas
    diagnostic_dict = {
        'recall' : tp/p,
        'false_neg_rate' : fn/p,
        'false_pos_rate' : fp/n,
        'true_neg_rate' : tn/n,
        'positive_liklihood_ratio' : (tp/p)/(fp/n),
        'neg_liklihood_rate' : (fn/p)/(tn/n),
        'precision' : tp/pp,
        'false_omission_rate' : fn/pn,
        'false_discovery_rate' : fp/pp,
        'neg_pred_value' : tn/pn,
        'markedness' : (tp/pp)+(tn/pn)-1,
        'diagnostic_odds_ration' : ((tp/p)/(fp/n))/( (fn/p)/(tn/n)),
        'informedness' : (tp/p)+(tn/n)-1,
        'prevalence_threshold' : (sqrt((tp/p)*(fp/n))-(fp/n))/((tp/p)-(fp/n)),
        'prevalence' : p/(p+n),
        'accuracy' : (tp+tn)/(p+n),
        'balanced_accuracy' : ((tp/p)+(tn/n))/2,
        'F1_score' : 2*tp/(2*tp+fp+fn),
        'fowlkes_mallows_index' : sqrt((tp/pp)*(tp/p)),
        'jaccard_index' : tp/(tp+fn+fp),
    }

    return diagnostic_dict

"""construcción de una matriz de confusión, donde se pasa las etiquetas originales de los datos <code>y_test</code> junto con los valores predecidos por el modelo <code>pred</code>"""

from sklearn.metrics import confusion_matrix

# X_test tiene el mismo formato de los datos de entrenamiento
# IMPORTANTE: Para utilizar el módelo para predecir, los datos deben contener la aplicación
# de todo el preprocesamiento efectuado a los datos utilizados para entrenar el modelo
pred = lr_model_fit.predict(X_test)

# se crea una matriz de confusión
cm = confusion_matrix(y_test,pred)

# El modelo entrega las etiquetas predecidas
# para el conjunto de predictores entregados
pred

# estas son las etiquetas originales
y_test.to_numpy()

# Esta es la matriz de confusión
# x  0  1
# 0  ?  ?
# 1  ?  ?
cm

"""¿Cual es la clase que mas le cuesta predecir al modelo?"""

# Se aplica la función para obtener las métricas a partir de la matriz de confusión
stats = model_diagnostic_stats(cm)
stats

#De esta forma obtengo solo el accuracy
stats['accuracy']

# Efectuamos una comparación con el modelo obtenido mediante búsquedad exhaustiva
# y el creado posteriormente ingresando los parametros obtenidos
pred2 = lr_model_clf.predict(X_test)
cm2 = confusion_matrix(y_test,pred2)
model_diagnostic_stats(cm2)

lr_model_clf.score(X_test,y_test)

"""# Comparación con otros algoritmos de clasificación

Se evaluaran otros módelos de clasificación que se pueden aplicar a este tipo de problemas.

## Random Forest
"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=100,
    min_samples_split=25,
    max_depth=7,
    max_features=2)

rf_model_fit = rf_model.fit(X_train,y_train)

pred_rf = rf_model_fit.predict(X_test)

cm_rf = confusion_matrix(y_test,pred_rf)
cm_rf

rf_stats = model_diagnostic_stats(cm_rf)
rf_stats['accuracy']

"""- Importancia de predictores"""

pd.concat((pd.DataFrame(X_train.iloc[:, 1:].columns, columns = ['variable']),
           pd.DataFrame(rf_model_fit.feature_importances_, columns = ['importance'])),
          axis = 1).sort_values(by='importance', ascending = False)[:20]

"""### Gráfico de RandomForest

Los árboles de decisión se utilizan cuando aparte de predecir, lo que se busca es, explicar la problematica mediante los atributos.
"""

# Obtenemos los árboles del random forest
arboles = rf_model_fit.estimators_

# Cantidad de árboles
# Corresponde al hiperparametro n_estimators=100,
len(arboles)

from sklearn.tree import plot_tree

#Gráficaremos solo 2 árbol de los 100
fig, ax = plt.subplots(figsize=(15, 10))
plot = plot_tree(
            decision_tree = arboles[0],
            feature_names = X_train.iloc[:, 1:].columns,
            class_names   = 'Diagnostico',
            filled        = True,
            impurity      = False,
            ax            = ax
       )

fig, ax = plt.subplots(figsize=(15, 10))
plot = plot_tree(
            decision_tree = arboles[26],
            feature_names = X_train.iloc[:, 1:].columns,
            class_names   = 'Diagnostico',
            filled        = True,
            impurity      = False,
            ax            = ax
       )

"""## SVM


"""

from sklearn.svm import SVC

svc_model = SVC(C = 100, kernel = 'linear', random_state=123)
svc_model_fit = svc_model.fit(X_train, y_train)

svc_pred = svc_model_fit.predict(X_test)

svc_cm = confusion_matrix(y_test,svc_pred)

svc_stats = model_diagnostic_stats(svc_cm)
svc_stats['accuracy']

"""# Conclusión

- Regresión Logística: 97,66%

Los siguientes algoritmos se aplicaron con hiperparametros por defecto:
- RandomForest: 94,15%
- SVC: 96,49%
"""